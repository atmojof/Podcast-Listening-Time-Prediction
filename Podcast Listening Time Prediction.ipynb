{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podcast Listening Time Prediction\n",
    "## Advanced Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv(\"/kaggle/input/playground-series-s5e4/train.csv\")\n",
    "test_df = pd.read_csv(\"/kaggle/input/playground-series-s5e4/test.csv\")\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/playground-series-s5e4/sample_submission.csv\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train_df['Listening_Time_minutes'], kde=True)\n",
    "plt.title('Distribution of Listening Time (minutes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "numeric_cols = train_df.select_dtypes(include=np.number).columns\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(train_df[numeric_cols].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "y = train_df['Listening_Time_minutes']\n",
    "drop_cols = ['id', 'Episode_Title', 'Listening_Time_minutes']\n",
    "X = train_df.drop(columns=drop_cols)\n",
    "X_test = test_df.drop(columns=['id', 'Episode_Title'])\n",
    "\n",
    "# Ordinal encoding for Publication_Time\n",
    "time_order = ['Early Morning', 'Morning', 'Afternoon', 'Evening', 'Night', 'Late Night']\n",
    "time_mapping = {time: i for i, time in enumerate(time_order)}\n",
    "X['Publication_Time_Ordinal'] = X['Publication_Time'].map(time_mapping)\n",
    "X_test['Publication_Time_Ordinal'] = X_test['Publication_Time'].map(time_mapping)\n",
    "\n",
    "# Text-based feature\n",
    "X['Episode_Title_Length'] = train_df['Episode_Title'].str.len()\n",
    "X_test['Episode_Title_Length'] = test_df['Episode_Title'].str.len()\n",
    "\n",
    "# Interaction features\n",
    "X['Duration_Sentiment_Interaction'] = X['Duration_minutes'] * X['Episode_Sentiment_Score']\n",
    "X_test['Duration_Sentiment_Interaction'] = X_test['Duration_minutes'] * X_test['Episode_Sentiment_Score']\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_cols = ['Podcast_Name', 'Genre', 'Publication_Day', 'Episode_Sentiment', 'Publication_Time']\n",
    "numerical_cols = [col for col in X.columns if col not in categorical_cols + ['Publication_Time_Ordinal']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        ('time_ordinal', 'passthrough', ['Publication_Time_Ordinal'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    'XGBoost': XGBRegressor(random_state=42, n_jobs=-1, eval_metric='rmse'),\n",
    "    'LightGBM': LGBMRegressor(random_state=42, n_jobs=-1),\n",
    "    'ElasticNet': ElasticNet(random_state=42)\n",
    "}\n",
    "\n",
    "# Hyperparameter grids\n",
    "param_grids = {\n",
    "    'RandomForest': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__max_depth': [None, 10, 20],\n",
    "        'model__min_samples_split': [2, 5]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model__n_estimators': [100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1],\n",
    "        'model__max_depth': [3, 6]\n",
    "    }\n",
    "}\n",
    "\n",
    "# KFold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n=== Training {name} ===\")\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('feature_selector', SelectFromModel(RandomForestRegressor(n_estimators=50, random_state=42))),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Hyperparameter tuning if parameters are defined\n",
    "    if name in param_grids:\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grids[name],\n",
    "            cv=kf,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        grid_search.fit(X, y)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_score = -grid_search.best_score_\n",
    "        best_params = grid_search.best_params_\n",
    "        \n",
    "        print(f\"Best RMSE: {best_score:.4f}\")\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': best_model,\n",
    "            'rmse': best_score,\n",
    "            'params': best_params\n",
    "        }\n",
    "    else:\n",
    "        # For models without hyperparameter tuning\n",
    "        scores = cross_val_score(\n",
    "            pipeline,\n",
    "            X,\n",
    "            y,\n",
    "            cv=kf,\n",
    "            scoring='neg_root_mean_squared_error',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        avg_rmse = -scores.mean()\n",
    "        \n",
    "        print(f\"Avg RMSE: {avg_rmse:.4f}\")\n",
    "        \n",
    "        # Fit the model on full data for later use\n",
    "        pipeline.fit(X, y)\n",
    "        results[name] = {\n",
    "            'model': pipeline,\n",
    "            'rmse': avg_rmse\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': results.keys(),\n",
    "    'RMSE': [result['rmse'] for result in results.values()]\n",
    "}).sort_values('RMSE')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='RMSE', y='Model', data=model_comparison, palette='viridis')\n",
    "plt.title('Model Comparison by RMSE')\n",
    "plt.xlabel('Root Mean Squared Error')\n",
    "plt.ylabel('Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ensemble Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top performing models\n",
    "top_models = sorted(results.items(), key=lambda x: x[1]['rmse'])[:3]\n",
    "\n",
    "# Create weighted ensemble predictions\n",
    "test_predictions = []\n",
    "weights = []\n",
    "\n",
    "for name, result in top_models:\n",
    "    pred = result['model'].predict(X_test)\n",
    "    test_predictions.append(pred)\n",
    "    # Inverse weight by RMSE (better models get more weight)\n",
    "    weights.append(1 / result['rmse'])\n",
    "\n",
    "# Normalize weights\n",
    "weights = np.array(weights) / sum(weights)\n",
    "\n",
    "# Create final ensemble prediction\n",
    "final_preds = np.average(test_predictions, axis=0, weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names = (numerical_cols + \n",
    "                 list(results['RandomForest']['model'].named_steps['preprocessor']\n",
    "                     .named_transformers_['cat']\n",
    "                     .named_steps['onehot']\n",
    "                     .get_feature_names_out(categorical_cols)) +\n",
    "                 ['Publication_Time_Ordinal'])\n",
    "\n",
    "# Display feature importance from best model\n",
    "best_model_name = top_models[0][0]\n",
    "if hasattr(results[best_model_name]['model'].named_steps['model'], 'feature_importances_'):\n",
    "    importances = results[best_model_name]['model'].named_steps['model'].feature_importances_\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', \n",
    "                data=importance_df.head(20), \n",
    "                palette='rocket')\n",
    "    plt.title(f'Top 20 Features by Importance ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare submission\n",
    "sample_submission['Listening_Time_minutes'] = final_preds\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved!\")\n",
    "\n",
    "# Show sample of predictions\n",
    "sample_submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
